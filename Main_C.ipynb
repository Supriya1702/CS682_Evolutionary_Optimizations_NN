{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UxO3GSllVMen_t7f9rIMUF8VbDFy7UHD","timestamp":1733008071166}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8EdF2UL_5mt","executionInfo":{"status":"ok","timestamp":1733002589737,"user_tz":300,"elapsed":5538,"user":{"displayName":"Aarushi Mahajan","userId":"11130209034004800611"}},"outputId":"dbb24766-a0c9-40ca-eaa3-a8a301e41933"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deap\n","  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.26.4)\n","Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deap\n","Successfully installed deap-1.4.1\n"]}],"source":["!pip install deap"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","from deap import base, creator, tools\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Set random seeds for reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","torch.manual_seed(42)\n","\n","# Data Loading, Preprocessing, and Splitting Function\n","def load_data(limit_samples=10000):\n","    print(\"Loading and preprocessing CIFAR-10 dataset...\")\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    # Normalize pixel values to [0, 1]\n","    x_train = x_train.astype('float32') / 255.0\n","    x_test = x_test.astype('float32') / 255.0\n","\n","    # Convert labels to one-hot encoding\n","    y_train = to_categorical(y_train, 10)\n","    y_test = to_categorical(y_test, 10)\n","\n","    # Limit the training dataset size\n","    x_train = x_train[:limit_samples]\n","    y_train = y_train[:limit_samples]\n","\n","    # Perform 60/20/20 split\n","    total_samples = len(x_train)\n","    train_size = int(0.6 * total_samples)\n","    val_size = int(0.2 * total_samples)\n","\n","    # Split the data\n","    x_train_split = x_train[:train_size]\n","    y_train_split = y_train[:train_size]\n","    x_val_split = x_train[train_size:train_size + val_size]\n","    y_val_split = y_train[train_size:train_size + val_size]\n","    x_test_split = x_test[:val_size]\n","    y_test_split = y_test[:val_size]\n","\n","    print(f\"Data split into: {train_size} train, {val_size} validation, {len(x_test_split)} test samples.\")\n","\n","    # Convert to PyTorch tensors\n","    train_dataset = TensorDataset(\n","        torch.tensor(x_train_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_train_split, axis=1)).long()\n","    )\n","    val_dataset = TensorDataset(\n","        torch.tensor(x_val_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_val_split, axis=1)).long()\n","    )\n","    test_dataset = TensorDataset(\n","        torch.tensor(x_test_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_test_split, axis=1)).long()\n","    )\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# Load data with a limit of 10,000 samples for training and validation\n","train_loader, val_loader, test_loader = load_data(limit_samples=30000)\n","\n","# Define DEAP's Fitness and Individual\n","if \"FitnessMulti\" not in creator.__dict__:\n","    creator.create(\"FitnessMulti\", base.Fitness, weights=(2.0, -0.5, -1.0))\n","\n","if \"Individual\" not in creator.__dict__:\n","    creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n","\n","# Generate random CNN architecture\n","def generate_individual():\n","    layers = []\n","    num_layers = random.randint(3, 10)  # 3-10 layers in the architecture\n","\n","    for _ in range(num_layers):\n","        layer_type = random.choice(['conv', 'batchnorm', 'dropout', 'dense'])\n","\n","        if layer_type == 'conv':\n","            layers.append({\n","                'type': 'conv',\n","                'filters': random.choice([16, 32, 64]),\n","                'kernel_size': random.choice([3, 5]),\n","                'stride': random.choice([1, 2]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","        elif layer_type == 'batchnorm':\n","            layers.append({'type': 'batchnorm'})\n","        elif layer_type == 'dropout':\n","            layers.append({'type': 'dropout', 'rate': random.uniform(0.1, 0.5)})\n","        elif layer_type == 'dense':\n","            layers.append({\n","                'type': 'dense',\n","                'units': random.choice([64, 128, 256]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","\n","    # Ensure a dense layer at the end\n","    layers.append({'type': 'dense', 'units': 10, 'activation': 'softmax'})\n","\n","    return creator.Individual(layers)\n","\n","# Define CNN Model\n","class CustomCNN(nn.Module):\n","    def __init__(self, architecture):\n","        super(CustomCNN, self).__init__()\n","        layers = []\n","        in_channels = 3\n","        current_height, current_width = 32, 32\n","        flattened = False\n","\n","        for layer in architecture:\n","            if layer['type'] == 'conv' and not flattened:\n","                kernel_size = layer['kernel_size']\n","                stride = layer['stride']\n","                padding = (kernel_size - 1) // 2\n","\n","                layers.append(\n","                    nn.Conv2d(in_channels, layer['filters'], kernel_size=kernel_size, stride=stride, padding=padding)\n","                )\n","                if layer['activation'] == 'relu':\n","                    layers.append(nn.ReLU())\n","                elif layer['activation'] == 'tanh':\n","                    layers.append(nn.Tanh())\n","\n","                in_channels = layer['filters']\n","                current_height = (current_height + 2 * padding - kernel_size) // stride + 1\n","                current_width = (current_width + 2 * padding - kernel_size) // stride + 1\n","\n","            elif layer['type'] == 'batchnorm' and not flattened:\n","                layers.append(nn.BatchNorm2d(in_channels))\n","\n","            elif layer['type'] == 'dropout':\n","                layers.append(nn.Dropout(layer['rate']))\n","\n","            elif layer['type'] == 'dense':\n","                if not flattened:\n","                    layers.append(nn.Flatten())\n","                    flattened = True\n","                    in_features = current_height * current_width * in_channels\n","\n","                layers.append(nn.Linear(in_features, layer['units']))\n","                in_features = layer['units']\n","\n","                if layer['activation'] == 'relu':\n","                    layers.append(nn.ReLU())\n","                elif layer['activation'] == 'tanh':\n","                    layers.append(nn.Tanh())\n","\n","        if not flattened:\n","            layers.append(nn.Flatten())\n","            in_features = current_height * current_width * in_channels\n","        layers.append(nn.Linear(in_features, 10))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define Mutation\n","def mutate(individual):\n","    if random.random() < 0.5:\n","        layer_idx = random.randint(0, len(individual) - 2)\n","        layer = individual[layer_idx]\n","        if layer['type'] == 'conv':\n","            layer['filters'] = random.choice([16, 32, 64])\n","            layer['kernel_size'] = random.choice([3, 5])\n","            layer['stride'] = random.choice([1, 2])\n","            layer['activation'] = random.choice(['relu', 'tanh'])\n","        elif layer['type'] == 'dropout':\n","            layer['rate'] = random.uniform(0.1, 0.5)\n","        elif layer['type'] == 'dense':\n","            layer['units'] = random.choice([64, 128, 256])\n","            layer['activation'] = random.choice(['relu', 'tanh'])\n","    else:\n","        if random.random() < 0.5 and len(individual) > 3:\n","            layer_idx = random.randint(0, len(individual) - 2)\n","            del individual[layer_idx]\n","        else:\n","            new_layer = random.choice(generate_individual()[:-1])\n","            individual.insert(random.randint(0, len(individual) - 1), new_layer)\n","    return creator.Individual(individual)\n","\n","# Define Crossover\n","def crossover(ind1, ind2):\n","    point = random.randint(1, min(len(ind1), len(ind2)) - 1)\n","    child1 = creator.Individual(ind1[:point] + ind2[point:])\n","    child2 = creator.Individual(ind2[:point] + ind1[point:])\n","    return child1, child2\n","\n","# Register Mutation and Crossover\n","toolbox = base.Toolbox()\n","toolbox.register(\"individual\", tools.initIterate, creator.Individual, generate_individual)\n","toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","toolbox.register(\"mate\", crossover)\n","toolbox.register(\"mutate\", mutate)\n","\n","# Evaluate Individual Fitness\n","def evaluate_cnn(individual):\n","    model = CustomCNN(individual).to(\"cpu\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(1):  # Train for 1 epoch\n","        model.train()\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(x_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for x_batch, y_batch in val_loader:\n","            outputs = model(x_batch)\n","            val_loss += criterion(outputs, y_batch).item()\n","            _, predicted = outputs.max(1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    accuracy = correct / total\n","    complexity = sum(p.numel() for p in model.parameters())\n","    return accuracy, complexity, val_loss / len(val_loader)\n","\n","toolbox.register(\"evaluate\", evaluate_cnn)\n","\n","# Lexicase Selection\n","def lexicase_selection(population, fitnesses):\n","    indices = list(range(len(population)))\n","    objectives = len(fitnesses[0])\n","    while len(indices) > 1:\n","        obj = random.randint(0, objectives - 1)\n","        best_value = max(fitnesses[i][obj] for i in indices)\n","        indices = [i for i in indices if fitnesses[i][obj] == best_value]\n","    return population[indices[0]]\n","\n","# Evolutionary Algorithm\n","def evolutionary_algorithm_lexicase(n_gen=3, pop_size=5, cxpb=0.7, mutpb=0.2):\n","    population = toolbox.population(n=pop_size)\n","    for gen in range(n_gen):\n","        print(f\"\\n-- Generation {gen} --\")\n","        fitnesses = list(map(toolbox.evaluate, population))\n","\n","        for ind, fit in zip(population, fitnesses):\n","            ind.fitness.values = fit\n","            print(f\"Individual Fitness: {fit}\")\n","\n","        offspring = []\n","        for _ in range(len(population)):\n","            selected = lexicase_selection(population, fitnesses)\n","            offspring.append(selected)\n","\n","        for i in range(1, len(offspring), 2):\n","            if random.random() < cxpb:\n","                offspring[i - 1], offspring[i] = toolbox.mate(offspring[i - 1], offspring[i])\n","        for i in range(len(offspring)):\n","            if random.random() < mutpb:\n","                offspring[i] = toolbox.mutate(offspring[i])\n","\n","        population[:] = offspring\n","\n","    return tools.selBest(population, 1)[0]\n","\n","# Run Evolutionary Algorithm\n","best_architecture_lexicase = evolutionary_algorithm_lexicase()\n","print(\"Best architecture using lexicase selection:\", best_architecture_lexicase)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Pco0xch_8bL","executionInfo":{"status":"ok","timestamp":1733004018475,"user_tz":300,"elapsed":411573,"user":{"displayName":"Aarushi Mahajan","userId":"11130209034004800611"}},"outputId":"cfabd91f-509e-4339-84a7-8c38892af0d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing CIFAR-10 dataset...\n","Data split into: 18000 train, 6000 validation, 6000 test samples.\n","\n","-- Generation 0 --\n","\n","-- Generation 1 --\n","\n","-- Generation 2 --\n","Best architecture using lexicase selection: [{'type': 'conv', 'filters': 16, 'kernel_size': 5, 'stride': 2, 'activation': 'relu'}, {'type': 'dense', 'units': 128, 'activation': 'tanh'}, {'type': 'batchnorm'}, {'type': 'dropout', 'rate': 0.3110695149673016}, {'type': 'dropout', 'rate': 0.19204589303863082}, {'type': 'conv', 'filters': 16, 'kernel_size': 3, 'stride': 2, 'activation': 'tanh'}, {'type': 'dropout', 'rate': 0.12647544208461892}, {'type': 'dropout', 'rate': 0.18505061762165625}, {'type': 'dense', 'units': 10, 'activation': 'softmax'}]\n"]}]},{"cell_type":"code","source":["def test_model(architecture, train_loader, test_loader):\n","    \"\"\"\n","    Fine-tune and evaluate the model on the test set.\n","\n","    Parameters:\n","    - architecture: The CNN architecture to be tested.\n","    - train_loader: DataLoader for the training set.\n","    - test_loader: DataLoader for the test set.\n","\n","    Prints:\n","    - Test accuracy after fine-tuning.\n","    \"\"\"\n","    model = CustomCNN(architecture).to(\"cpu\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    print(\"Fine-tuning the model on the training set...\")\n","\n","    # Train the model\n","    for epoch in range(5):  # Fine-tune for 5 epochs\n","        model.train()\n","        running_loss = 0.0\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(x_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n","\n","    print(\"Evaluating the model on the test set...\")\n","\n","    # Evaluate on the test set\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x_batch, y_batch in test_loader:\n","            outputs = model(x_batch)\n","            _, predicted = outputs.max(1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    accuracy = correct / total\n","    print(f\"Test Accuracy: {accuracy:.4f}\")\n","    return accuracy\n","\n","# Test the best architecture from the evolutionary algorithm\n","print(\"Testing the best architecture from evolutionary algorithm...\")\n","test_accuracy = test_model(best_architecture_lexicase, train_loader, test_loader)\n","print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"La-8VBMEAWCH","executionInfo":{"status":"ok","timestamp":1733004050014,"user_tz":300,"elapsed":22829,"user":{"displayName":"Aarushi Mahajan","userId":"11130209034004800611"}},"outputId":"529ee315-5417-4b88-a852-6902e4736aee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing the best architecture from evolutionary algorithm...\n","Fine-tuning the model on the training set...\n","Epoch 1, Loss: 1.8576\n","Epoch 2, Loss: 1.5770\n","Epoch 3, Loss: 1.4859\n","Epoch 4, Loss: 1.4005\n","Epoch 5, Loss: 1.3461\n","Evaluating the model on the test set...\n","Test Accuracy: 0.5198\n","Final Test Accuracy: 0.5198\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1kdO_4PsFuPU"},"execution_count":null,"outputs":[]}]}