# ğŸ§¬ Evolutionary Optimization of Deep Neural Network Architectures

This project explores **gradient-free optimization** of Convolutional Neural Networks (CNNs) using **evolutionary algorithms** on the CIFAR-10 dataset. By integrating **tournament** and **lexicase selection**, this hybrid framework discovers optimized CNN architectures that balance **accuracy, complexity, and training time**.

---

## ğŸ“Œ Overview

**Team Members:**
- Supriya Kumari â€“ [supriyakumar@umass.edu](mailto:supriyakumar@umass.edu)
- Kavisha Ghodasara â€“ [kghodasara@umass.edu](mailto:kghodasara@umass.edu)
- Subhangi Choudhary â€“ [subhangichou@umass.edu](mailto:subhangichou@umass.edu)

**Course:** COMPSCI 682 - Neural Networks  
**Institution:** University of Massachusetts Amherst

---

## ğŸ§  Core Contributions

- **Evolutionary Architecture Search**: Hybrid selection with tournament (exploit early) and lexicase (diversify late).
- **Multi-objective Fitness Evaluation**:
  - Accuracy
  - Model Complexity (params)
  - Training Time
- **Genetic Operations**:
  - Two-point crossover
  - Controlled mutation
  - Elitism for performance preservation

---

## ğŸ› ï¸ Tools & Frameworks

- [DEAP](https://github.com/DEAP/deap): Evolutionary algorithms
- TensorFlow / Keras: CNN definition & training
- NumPy, Matplotlib: Data manipulation & visualization

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ evolution/               # Evolution strategy (selection, crossover, mutation)
â”œâ”€â”€ cnn_models/              # CNN architecture generation
â”œâ”€â”€ data/                    # CIFAR-10 loading & preprocessing
â”œâ”€â”€ experiments/             # Experiment logs and result plots
â”œâ”€â”€ config.py                # Evolution hyperparameters
â”œâ”€â”€ main.py                  # Training & evolution execution script
â””â”€â”€ README.md
```

---

## ğŸ§ª Results Summary

| Experiment | Architecture | Test Accuracy | Test Loss |
|------------|--------------|----------------|-----------|
| Exp 1 | Conv(64, ReLU) â†’ Conv(128, Tanh) â†’ Dense(128, Tanh) | 64.7% | 1.87 |
| Exp 2 | 3 Ã— Conv(64) [ReLU/Tanh] | 67.9% | 0.95 |
| Exp 3 | 3 Ã— Conv(64) [Tanh â†’ ReLU â†’ ReLU] | 71.1% | 1.23 |
| Exp 4 | 2 Ã— Conv(64) + Dense(256, ReLU) | 58.6% | 3.59 |
| Exp 5 (weighted fitness) | 3 Ã— Conv(64, ReLU) | 70.17% | 0.96 |
| Exp 6 (weighted fitness) | Conv(16 â†’ 64 â†’ 64, mixed) | 63.88% | 2.70 |
| **With Elitism** | Conv(16 â†’ 64 â†’ 64, ReLU) | **73.82%** |  â€“ |

---

## ğŸ§¬ Genetic Strategy

- **Initialization**: Randomized CNN genome (filters, activations, layers)
- **Selection**:
  - Tournament (first half): Exploits best candidates
  - Lexicase (second half): Ensures diversity and Pareto-optimality
- **Crossover**: Two-point genetic recombination
- **Mutation**: Random layer or parameter change
- **Elitism**: Preserves top-k architectures each generation

---

## âš–ï¸ Fitness Configurations

| Scenario | Accuracy | Complexity | Time |
|----------|----------|------------|------|
| Equal Weighting | 1.0 | -1.0 | -1.0 |
| Varied (Exp 5) | 1.0 | -0.5 | -0.2 |
| Varied (Exp 6) | 1.5 | -1.0 | -0.75 |

---

## ğŸ“Š Comparative Evaluation

| Model      | Accuracy | Notes |
|------------|----------|-------|
| **Evolved CNN (Elitism)** | **73.82%** | Compact, evolved |
| Hand-Crafted CNN | 75.02% | Manual design with Dropout, BatchNorm |
| ResNet50 (Transfer Learning) | 25.27% | Overfitted due to small image size |

---

## ğŸš€ How to Run

```bash
# Clone repository
git clone https://github.com/Supriya1702/CS682_Evolutionary_Optimizations_NN.git
cd CS682_Evolutionary_Optimizations_NN

# Install dependencies
pip install -r requirements.txt

# Run evolution
python main.py --generations 10 --pop_size 20 --elitism True
```

---

## ğŸ“Œ Key Takeaways

- Evolutionary algorithms can **automate CNN design** with multi-objective trade-offs.
- **Elitism and hybrid selection** enhance robustness and convergence.
- Efficient architectures can be discovered **without manual tuning**.

---

## ğŸ“– Citation

```
Supriya Kumari, Kavisha Ghodasara, Subhangi Choudhary. 
"Evolutionary Optimization of Deep Neural Network Architectures", Final Project â€“ COMPSCI 682, UMass Amherst, Spring 2025.
```

--- 
