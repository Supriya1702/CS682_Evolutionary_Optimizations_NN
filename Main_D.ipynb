{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install deap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYo_v8TX81iM","outputId":"95dc4e7e-941e-49a1-a89f-03513c6e568d","executionInfo":{"status":"ok","timestamp":1733007467300,"user_tz":300,"elapsed":3377,"user":{"displayName":"Rahul shah","userId":"08697968008726674087"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deap\n","  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.26.4)\n","Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/135.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deap\n","Successfully installed deap-1.4.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkjrkLo-8z9J","outputId":"eba39910-f84d-4776-b6df-373f7b05005e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing CIFAR-10 dataset...\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n","Data preparation complete.\n","Training data shape: (900, 32, 32, 3)\n","Validation data shape: (100, 32, 32, 3)\n","Test data shape: (10000, 32, 32, 3)\n","\n","-- Generation 0 --\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3500, Complexity = 78298, Validation Loss = 1.6581\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 50698, Validation Loss = 1.8692\n","Evaluating individual: [{'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.2100, Complexity = 789258, Validation Loss = 2.0699\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.2400, Complexity = 205642, Validation Loss = 2.0573\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.1900, Complexity = 215882, Validation Loss = 2.0829\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3700, Complexity = 301578, Validation Loss = 1.8244\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.2500, Complexity = 201482, Validation Loss = 2.2338\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 128, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4100, Complexity = 533770, Validation Loss = 1.7845\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4400, Complexity = 1061130, Validation Loss = 1.9180\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3100, Complexity = 215882, Validation Loss = 2.0993\n","Individual Fitness: (0.35, 78298, 1.6580669283866882)\n","Individual Fitness: (0.45, 50698, 1.869215965270996)\n","Individual Fitness: (0.21, 789258, 2.069889932870865)\n","Individual Fitness: (0.24, 205642, 2.0572574734687805)\n","Individual Fitness: (0.19, 215882, 2.0829034745693207)\n","Individual Fitness: (0.37, 301578, 1.8244415521621704)\n","Individual Fitness: (0.25, 201482, 2.2337602376937866)\n","Individual Fitness: (0.41, 533770, 1.7844659388065338)\n","Individual Fitness: (0.44, 1061130, 1.9179833829402924)\n","Individual Fitness: (0.31, 215882, 2.099277585744858)\n","\n","-- Generation 1 --\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3700, Complexity = 1090122, Validation Loss = 2.2510\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 77274, Validation Loss = 1.9726\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4000, Complexity = 21274, Validation Loss = 1.7551\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4100, Complexity = 50698, Validation Loss = 1.8113\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4400, Complexity = 50698, Validation Loss = 1.8244\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3200, Complexity = 215882, Validation Loss = 2.0640\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3900, Complexity = 1061130, Validation Loss = 2.2218\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 50698, Validation Loss = 1.7634\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 1051850, Validation Loss = 2.1262\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4600, Complexity = 50698, Validation Loss = 1.7555\n","Individual Fitness: (0.37, 1090122, 2.2510473132133484)\n","Individual Fitness: (0.43, 77274, 1.9726307690143585)\n","Individual Fitness: (0.4, 21274, 1.7551133930683136)\n","Individual Fitness: (0.41, 50698, 1.8113199174404144)\n","Individual Fitness: (0.44, 50698, 1.8243703544139862)\n","Individual Fitness: (0.32, 215882, 2.0640028417110443)\n","Individual Fitness: (0.39, 1061130, 2.2217541933059692)\n","Individual Fitness: (0.43, 50698, 1.763439565896988)\n","Individual Fitness: (0.45, 1051850, 2.1261530220508575)\n","Individual Fitness: (0.46, 50698, 1.7555226683616638)\n","\n","-- Generation 2 --\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3700, Complexity = 50522, Validation Loss = 1.7192\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3800, Complexity = 1051850, Validation Loss = 2.2078\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 1061130, Validation Loss = 1.8855\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4600, Complexity = 77274, Validation Loss = 1.9267\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4600, Complexity = 50698, Validation Loss = 1.8317\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4600, Complexity = 50698, Validation Loss = 1.6938\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.2300, Complexity = 197322, Validation Loss = 2.0192\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4700, Complexity = 77274, Validation Loss = 1.7448\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4400, Complexity = 77274, Validation Loss = 1.9027\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4100, Complexity = 1051850, Validation Loss = 2.0539\n","Individual Fitness: (0.37, 50522, 1.7191891968250275)\n","Individual Fitness: (0.38, 1051850, 2.2077779173851013)\n","Individual Fitness: (0.45, 1061130, 1.885537713766098)\n","Individual Fitness: (0.46, 77274, 1.9266821146011353)\n","Individual Fitness: (0.46, 50698, 1.831726312637329)\n","Individual Fitness: (0.46, 50698, 1.6938284039497375)\n","Individual Fitness: (0.23, 197322, 2.0191641747951508)\n","Individual Fitness: (0.47, 77274, 1.744837760925293)\n","Individual Fitness: (0.44, 77274, 1.902717649936676)\n","Individual Fitness: (0.41, 1051850, 2.0539169311523438)\n","\n","-- Generation 3 --\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4200, Complexity = 77274, Validation Loss = 1.9591\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 77274, Validation Loss = 1.7698\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3900, Complexity = 1061130, Validation Loss = 1.8635\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 50698, Validation Loss = 1.6862\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4100, Complexity = 77274, Validation Loss = 1.9702\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4000, Complexity = 77274, Validation Loss = 1.8290\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4100, Complexity = 13018, Validation Loss = 1.7800\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4600, Complexity = 301578, Validation Loss = 2.3402\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 13018, Validation Loss = 1.7978\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 301578, Validation Loss = 2.2252\n","Individual Fitness: (0.42, 77274, 1.9590716361999512)\n","Individual Fitness: (0.43, 77274, 1.7698465883731842)\n","Individual Fitness: (0.39, 1061130, 1.8634766042232513)\n","Individual Fitness: (0.43, 50698, 1.6862480342388153)\n","Individual Fitness: (0.41, 77274, 1.9701701700687408)\n","Individual Fitness: (0.4, 77274, 1.8290155529975891)\n","Individual Fitness: (0.41, 13018, 1.7800408005714417)\n","Individual Fitness: (0.46, 301578, 2.3402107059955597)\n","Individual Fitness: (0.45, 13018, 1.797784000635147)\n","Individual Fitness: (0.43, 301578, 2.2252170145511627)\n","\n","-- Generation 4 --\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4000, Complexity = 13018, Validation Loss = 1.8136\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.2000, Complexity = 197322, Validation Loss = 2.0965\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3900, Complexity = 77274, Validation Loss = 1.8573\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3900, Complexity = 13018, Validation Loss = 1.7169\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4200, Complexity = 50698, Validation Loss = 1.7389\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4200, Complexity = 50698, Validation Loss = 1.7332\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 77274, Validation Loss = 1.8021\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3800, Complexity = 301578, Validation Loss = 2.1161\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 77274, Validation Loss = 1.8029\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3800, Complexity = 263306, Validation Loss = 1.8949\n","Individual Fitness: (0.4, 13018, 1.8136163651943207)\n","Individual Fitness: (0.2, 197322, 2.0965128242969513)\n","Individual Fitness: (0.39, 77274, 1.8572825193405151)\n","Individual Fitness: (0.39, 13018, 1.716871827840805)\n","Individual Fitness: (0.42, 50698, 1.738936722278595)\n","Individual Fitness: (0.42, 50698, 1.7332217693328857)\n","Individual Fitness: (0.43, 77274, 1.8021495342254639)\n","Individual Fitness: (0.38, 301578, 2.116075962781906)\n","Individual Fitness: (0.43, 77274, 1.8029280304908752)\n","Individual Fitness: (0.38, 263306, 1.8948528170585632)\n","\n","-- Generation 5 --\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3200, Complexity = 263306, Validation Loss = 1.9020\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4400, Complexity = 77274, Validation Loss = 2.0980\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4200, Complexity = 50698, Validation Loss = 1.8016\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 13018, Validation Loss = 1.7907\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4500, Complexity = 50698, Validation Loss = 1.7906\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4000, Complexity = 1051082, Validation Loss = 1.9924\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 13018, Validation Loss = 1.7533\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.4300, Complexity = 301578, Validation Loss = 2.1840\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3900, Complexity = 77274, Validation Loss = 1.9497\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3700, Complexity = 50698, Validation Loss = 1.7205\n","Individual Fitness: (0.32, 263306, 1.9020245373249054)\n","Individual Fitness: (0.44, 77274, 2.0979771316051483)\n","Individual Fitness: (0.42, 50698, 1.801595777273178)\n","Individual Fitness: (0.45, 13018, 1.7907448410987854)\n","Individual Fitness: (0.45, 50698, 1.7906323075294495)\n","Individual Fitness: (0.4, 1051082, 1.9923829138278961)\n","Individual Fitness: (0.43, 13018, 1.7533243894577026)\n","Individual Fitness: (0.43, 301578, 2.184012919664383)\n","Individual Fitness: (0.39, 77274, 1.9496966302394867)\n","Individual Fitness: (0.37, 50698, 1.7204585373401642)\n","Best Individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}], Fitness: (0.45, 13018.0, 1.7907448410987854)\n","Best architecture: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","from deap import base, creator, tools, algorithms\n","import random\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Set random seeds for reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","torch.manual_seed(42)\n","\n","# Define multi-objective fitness to maximize accuracy, minimize complexity, and training time\n","creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0, -1.0))  # Maximize accuracy, minimize complexity & time\n","creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n","\n","# Data Loading and Preprocessing\n","def load_data():\n","    print(\"Loading and preprocessing CIFAR-10 dataset...\")\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    # Normalize pixel values to [0, 1]\n","    x_train = x_train.astype('float32') / 255.0\n","    x_test = x_test.astype('float32') / 255.0\n","\n","    # Convert labels to one-hot encoding\n","    y_train = to_categorical(y_train, 10)\n","    y_test = to_categorical(y_test, 10)\n","\n","    # Split training data into training and validation sets\n","    val_split = 0.1\n","    num_sample = 1000\n","    x_train = x_train[:num_sample]\n","    y_train = y_train[:num_sample]\n","    val_size = int(len(x_train) * val_split)\n","    x_val, y_val = x_train[:val_size], y_train[:val_size]\n","    x_train, y_train = x_train[val_size:], y_train[val_size:]\n","\n","    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n","\n","(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()\n","print(\"Data preparation complete.\")\n","print(\"Training data shape:\", x_train.shape)\n","print(\"Validation data shape:\", x_val.shape)\n","print(\"Test data shape:\", x_test.shape)\n","\n","# Convert data to PyTorch format\n","train_dataset = TensorDataset(\n","    torch.tensor(x_train).permute(0, 3, 1, 2).float(), torch.tensor(np.argmax(y_train, axis=1)).long()\n",")\n","val_dataset = TensorDataset(\n","    torch.tensor(x_val).permute(0, 3, 1, 2).float(), torch.tensor(np.argmax(y_val, axis=1)).long()\n",")\n","test_dataset = TensorDataset(\n","    torch.tensor(x_test).permute(0, 3, 1, 2).float(), torch.tensor(np.argmax(y_test, axis=1)).long()\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Define Early Stopping\n","class EarlyStopping:\n","    def __init__(self, patience=5, delta=0.01):\n","        self.patience = patience\n","        self.delta = delta\n","        self.best_loss = float('inf')\n","        self.counter = 0\n","        self.early_stop = False\n","\n","    def __call__(self, validation_loss):\n","        if validation_loss < self.best_loss - self.delta:\n","            self.best_loss = validation_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","\n","# Neural Network Architecture Generation\n","def generate_individual():\n","    layers = []\n","    num_layers = random.randint(2, 4)  # Random number of layers\n","    for _ in range(num_layers):\n","        layer_type = random.choice(['conv', 'dense'])\n","        if layer_type == 'conv':\n","            layers.append({\n","                'type': 'conv',\n","                'filters': random.choice([16, 32, 64]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","        elif layer_type == 'dense':\n","            layers.append({\n","                'type': 'dense',\n","                'units': random.choice([64, 128, 256]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","    return layers\n","\n","toolbox = base.Toolbox()\n","toolbox.register(\"individual\", tools.initIterate, creator.Individual, generate_individual)\n","toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","\n","class CustomCNN(nn.Module):\n","    def __init__(self, architecture):\n","        super(CustomCNN, self).__init__()\n","        self.layers = nn.ModuleList()\n","        in_channels = 3  # Input channels for CIFAR-10 (RGB)\n","        current_height, current_width = 32, 32  # CIFAR-10 image dimensions\n","        added_flatten = False\n","        input_size = None  # Initialize input_size to avoid referencing before assignment\n","\n","        for layer in architecture:\n","            if layer['type'] == 'conv' and not added_flatten:\n","                self.layers.append(\n","                    nn.Conv2d(in_channels, layer['filters'], kernel_size=3, padding=1)\n","                )\n","                self.layers.append(nn.ReLU() if layer['activation'] == 'relu' else nn.Tanh())\n","                self.layers.append(nn.MaxPool2d(kernel_size=2))\n","                in_channels = layer['filters']\n","                # Update spatial dimensions after pooling\n","                current_height //= 2\n","                current_width //= 2\n","            elif layer['type'] == 'dense':\n","                if not added_flatten:\n","                    # Dynamically compute the input size for the first dense layer\n","                    input_size = current_height * current_width * in_channels\n","                    self.layers.append(nn.Flatten())\n","                    added_flatten = True\n","                # Add dense layers\n","                self.layers.append(nn.Linear(input_size, layer['units']))\n","                self.layers.append(nn.ReLU() if layer['activation'] == 'relu' else nn.Tanh())\n","                input_size = layer['units']\n","\n","        # Ensure Flatten and Input Size Calculation\n","        if not added_flatten:\n","            self.layers.append(nn.Flatten())\n","            input_size = current_height * current_width * in_channels  # Compute for first Linear layer\n","\n","        # Add output layer\n","        self.layers.append(nn.Linear(input_size, 10))  # 10 output classes for CIFAR-10\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","def evaluate_cnn(individual):\n","    print(f\"Evaluating individual: {individual}\")  # Log the individual's architecture\n","    model = CustomCNN(individual)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    early_stopping = EarlyStopping(patience=5)\n","\n","    for epoch in range(50):  # Train for 50 epochs or until early stopping\n","        model.train()\n","        train_loss = 0\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(x_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation loss\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in val_loader:\n","                outputs = model(x_batch)\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","        val_loss /= len(val_loader)\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","            break\n","\n","    # Calculate final accuracy\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x_batch, y_batch in val_loader:\n","            outputs = model(x_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    accuracy = correct / total\n","    complexity = sum(p.numel() for p in model.parameters())\n","    print(f\"Results for individual: Accuracy = {accuracy:.4f}, Complexity = {complexity}, Validation Loss = {val_loss:.4f}\")  # Log results\n","    return accuracy, complexity, val_loss\n","\n","toolbox.register(\"evaluate\", evaluate_cnn)\n","\n","# Evolutionary Algorithm\n","def evolutionary_algorithm(n_gen=6, pop_size=10, cxpb=0.7, mutpb=0.5):\n","    population = toolbox.population(n=pop_size)\n","    for gen in range(n_gen):\n","        print(f\"\\n-- Generation {gen} --\")\n","\n","        # Evaluate fitness of population\n","        fitnesses = list(map(toolbox.evaluate, population))\n","        for ind, fit in zip(population, fitnesses):\n","            ind.fitness.values = fit\n","            print(f\"Individual Fitness: {fit}\")  # Log fitness of each individual\n","\n","        # Select offspring\n","        offspring = tools.selTournament(population, len(population), tournsize=3)\n","        offspring = list(map(toolbox.clone, offspring))\n","\n","        # Apply crossover\n","        for i in range(1, len(offspring), 2):  # Iterate over pairs of individuals\n","            if random.random() < cxpb:\n","                tools.cxTwoPoint(offspring[i - 1], offspring[i])\n","\n","        # Apply mutation\n","        for mutant in offspring:\n","            if random.random() < mutpb:\n","                tools.mutShuffleIndexes(mutant, indpb=0.2)\n","\n","        # Replace population with new offspring\n","        population[:] = offspring\n","\n","    # Log final best individual\n","    best_individual = tools.selBest(population, 1)[0]\n","    print(f\"Best Individual: {best_individual}, Fitness: {best_individual.fitness.values}\")\n","    return best_individual\n","\n","# Run Evolutionary Algorithm\n","best_architecture = evolutionary_algorithm()\n","print(\"Best architecture:\", best_architecture)\n"]},{"cell_type":"markdown","source":["###Larger Data SubSet###\n","\n","Changing the Multifitness function for maximizing accuracy, minimizing complexity, and training time."],"metadata":{"id":"RapwnEQqlrwv"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","from deap import base, creator, tools, algorithms\n","import random\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Set random seeds for reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","torch.manual_seed(42)\n","\n","# Define multi-objective fitness to maximize accuracy, minimize complexity, and training time\n","creator.create(\"FitnessMulti\", base.Fitness, weights=(2.0, -0.5, -1.0))  # Maximize accuracy\n","creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n","\n","\n","# Data Loading, Preprocessing, and Splitting Function\n","def load_data(limit_samples=10000):\n","    \"\"\"\n","    Load and preprocess CIFAR-10 dataset with a limited number of samples.\n","    Performs a 60/20/20 split on the limited dataset.\n","\n","    Args:\n","        limit_samples (int): The total number of samples to use for training and validation.\n","\n","    Returns:\n","        tuple: PyTorch DataLoaders for training, validation, and test datasets.\n","    \"\"\"\n","    print(\"Loading and preprocessing CIFAR-10 dataset...\")\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    # Normalize pixel values to [0, 1]\n","    x_train = x_train.astype('float32') / 255.0\n","    x_test = x_test.astype('float32') / 255.0\n","\n","    # Convert labels to one-hot encoding\n","    y_train = to_categorical(y_train, 10)\n","    y_test = to_categorical(y_test, 10)\n","\n","    # Limit the training dataset size\n","    x_train = x_train[:limit_samples]\n","    y_train = y_train[:limit_samples]\n","\n","    # Perform 60/20/20 split\n","    total_samples = len(x_train)\n","    train_size = int(0.6 * total_samples)\n","    val_size = int(0.2 * total_samples)\n","\n","    # Split the data\n","    x_train_split = x_train[:train_size]\n","    y_train_split = y_train[:train_size]\n","    x_val_split = x_train[train_size:train_size + val_size]\n","    y_val_split = y_train[train_size:train_size + val_size]\n","    x_test_split = x_test[:val_size]  # Optionally limit the test set for consistency\n","    y_test_split = y_test[:val_size]\n","\n","    print(f\"Data split into: {train_size} train, {val_size} validation, {len(x_test_split)} test samples.\")\n","\n","    # Convert to PyTorch tensors\n","    train_dataset = TensorDataset(\n","        torch.tensor(x_train_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_train_split, axis=1)).long()\n","    )\n","    val_dataset = TensorDataset(\n","        torch.tensor(x_val_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_val_split, axis=1)).long()\n","    )\n","    test_dataset = TensorDataset(\n","        torch.tensor(x_test_split).permute(0, 3, 1, 2).float(),\n","        torch.tensor(np.argmax(y_test_split, axis=1)).long()\n","    )\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# Load data with a limit of 10,000 samples for training and validation\n","train_loader, val_loader, test_loader = load_data(limit_samples=10000)\n","\n","# Print DataLoader sizes\n","print(f\"Train batches: {len(train_loader)}\")\n","print(f\"Validation batches: {len(val_loader)}\")\n","print(f\"Test batches: {len(test_loader)}\")\n","\n","# Define Early Stopping\n","class EarlyStopping:\n","    def __init__(self, patience=5, delta=0.01):\n","        self.patience = patience\n","        self.delta = delta\n","        self.best_loss = float('inf')\n","        self.counter = 0\n","        self.early_stop = False\n","\n","    def __call__(self, validation_loss):\n","        if validation_loss < self.best_loss - self.delta:\n","            self.best_loss = validation_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","\n","# Neural Network Architecture Generation\n","def generate_individual():\n","    layers = []\n","    num_layers = random.randint(2, 4)  # Random number of layers\n","    for _ in range(num_layers):\n","        layer_type = random.choice(['conv', 'dense'])\n","        if layer_type == 'conv':\n","            layers.append({\n","                'type': 'conv',\n","                'filters': random.choice([16, 32, 64]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","        elif layer_type == 'dense':\n","            layers.append({\n","                'type': 'dense',\n","                'units': random.choice([64, 128, 256]),\n","                'activation': random.choice(['relu', 'tanh'])\n","            })\n","    return layers\n","\n","toolbox = base.Toolbox()\n","toolbox.register(\"individual\", tools.initIterate, creator.Individual, generate_individual)\n","toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","\n","class CustomCNN(nn.Module):\n","    def __init__(self, architecture):\n","        super(CustomCNN, self).__init__()\n","        self.layers = nn.ModuleList()\n","        in_channels = 3  # Input channels for CIFAR-10 (RGB)\n","        current_height, current_width = 32, 32  # CIFAR-10 image dimensions\n","        added_flatten = False\n","        input_size = None  # Initialize input_size to avoid referencing before assignment\n","\n","        for layer in architecture:\n","            if layer['type'] == 'conv' and not added_flatten:\n","                self.layers.append(\n","                    nn.Conv2d(in_channels, layer['filters'], kernel_size=3, padding=1)\n","                )\n","                self.layers.append(nn.ReLU() if layer['activation'] == 'relu' else nn.Tanh())\n","                self.layers.append(nn.MaxPool2d(kernel_size=2))\n","                in_channels = layer['filters']\n","                # Update spatial dimensions after pooling\n","                current_height //= 2\n","                current_width //= 2\n","            elif layer['type'] == 'dense':\n","                if not added_flatten:\n","                    # Dynamically compute the input size for the first dense layer\n","                    input_size = current_height * current_width * in_channels\n","                    self.layers.append(nn.Flatten())\n","                    added_flatten = True\n","                # Add dense layers\n","                self.layers.append(nn.Linear(input_size, layer['units']))\n","                self.layers.append(nn.ReLU() if layer['activation'] == 'relu' else nn.Tanh())\n","                input_size = layer['units']\n","\n","        # Ensure Flatten and Input Size Calculation\n","        if not added_flatten:\n","            self.layers.append(nn.Flatten())\n","            input_size = current_height * current_width * in_channels  # Compute for first Linear layer\n","\n","        # Add output layer\n","        self.layers.append(nn.Linear(input_size, 10))  # 10 output classes for CIFAR-10\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","def evaluate_cnn(individual):\n","    print(f\"Evaluating individual: {individual}\")  # Log the individual's architecture\n","    model = CustomCNN(individual)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    early_stopping = EarlyStopping(patience=10)\n","\n","    for epoch in range(50):  # Train for 50 epochs or until early stopping\n","        model.train()\n","        train_loss = 0\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(x_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation loss\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in val_loader:\n","                outputs = model(x_batch)\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","        val_loss /= len(val_loader)\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","            break\n","\n","    # Calculate final accuracy\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x_batch, y_batch in val_loader:\n","            outputs = model(x_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    accuracy = correct / total\n","    complexity = sum(p.numel() for p in model.parameters())\n","    print(f\"Results for individual: Accuracy = {accuracy:.4f}, Complexity = {complexity}, Validation Loss = {val_loss:.4f}\")  # Log results\n","    return accuracy, complexity, val_loss\n","\n","toolbox.register(\"evaluate\", evaluate_cnn)\n","\n","# Evolutionary Algorithm\n","def evolutionary_algorithm(n_gen=2, pop_size=10, cxpb=0.7, mutpb=0.2):\n","    population = toolbox.population(n=pop_size)\n","    for gen in range(n_gen):\n","        print(f\"\\n-- Generation {gen} --\")\n","\n","        # Evaluate fitness of population\n","        fitnesses = list(map(toolbox.evaluate, population))\n","        for ind, fit in zip(population, fitnesses):\n","            ind.fitness.values = fit\n","            print(f\"Individual Fitness: {fit}\")  # Log fitness of each individual\n","\n","        # Select offspring\n","        offspring = tools.selTournament(population, len(population), tournsize=3)\n","        offspring = list(map(toolbox.clone, offspring))\n","\n","        # Apply crossover\n","        for i in range(1, len(offspring), 2):  # Iterate over pairs of individuals\n","            if random.random() < cxpb:\n","                tools.cxTwoPoint(offspring[i - 1], offspring[i])\n","\n","        # Apply mutation\n","        for mutant in offspring:\n","            if random.random() < mutpb:\n","                tools.mutShuffleIndexes(mutant, indpb=0.2)\n","\n","        # Replace population with new offspring\n","        population[:] = offspring\n","\n","    # Log final best individual\n","    best_individual = tools.selBest(population, 1)[0]\n","    print(f\"Best Individual: {best_individual}, Fitness: {best_individual.fitness.values}\")\n","    return best_individual\n","\n","# Run Evolutionary Algorithm\n","best_architecture = evolutionary_algorithm()\n","print(\"Best architecture:\", best_architecture)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AdAC9Xu85OR","outputId":"737b5f09-2abb-4234-90f9-46e91ee73b86","executionInfo":{"status":"ok","timestamp":1733009792502,"user_tz":300,"elapsed":1487017,"user":{"displayName":"Rahul shah","userId":"08697968008726674087"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing CIFAR-10 dataset...\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","Data split into: 6000 train, 2000 validation, 2000 test samples.\n","Train batches: 188\n","Validation batches: 63\n","Test batches: 63\n","\n","-- Generation 0 --\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.5530, Complexity = 78298, Validation Loss = 1.4519\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5490, Complexity = 50698, Validation Loss = 1.5270\n","Evaluating individual: [{'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3475, Complexity = 789258, Validation Loss = 1.8444\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4105, Complexity = 205642, Validation Loss = 1.6907\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.3240, Complexity = 215882, Validation Loss = 1.8448\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5800, Complexity = 301578, Validation Loss = 1.6393\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.3530, Complexity = 201482, Validation Loss = 1.8462\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 128, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4975, Complexity = 533770, Validation Loss = 2.3439\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5390, Complexity = 1061130, Validation Loss = 2.1944\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4135, Complexity = 215882, Validation Loss = 1.6886\n","Individual Fitness: (0.553, 78298, 1.4518845100251456)\n","Individual Fitness: (0.549, 50698, 1.5269577947873918)\n","Individual Fitness: (0.3475, 789258, 1.8444086861988855)\n","Individual Fitness: (0.4105, 205642, 1.6906914824531192)\n","Individual Fitness: (0.324, 215882, 1.8447648021909926)\n","Individual Fitness: (0.58, 301578, 1.6393303237264119)\n","Individual Fitness: (0.353, 201482, 1.846158506378295)\n","Individual Fitness: (0.4975, 533770, 2.343868022873288)\n","Individual Fitness: (0.539, 1061130, 2.1943602268657987)\n","Individual Fitness: (0.4135, 215882, 1.6885964946141319)\n","\n","-- Generation 1 --\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5765, Complexity = 1090122, Validation Loss = 1.9895\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.5575, Complexity = 77274, Validation Loss = 1.6716\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.5560, Complexity = 78298, Validation Loss = 1.4477\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5490, Complexity = 50698, Validation Loss = 1.5758\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5735, Complexity = 50698, Validation Loss = 1.3542\n","Evaluating individual: [{'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.4025, Complexity = 215882, Validation Loss = 1.6832\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.5675, Complexity = 22298, Validation Loss = 1.4482\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'tanh'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}]\n","Results for individual: Accuracy = 0.5500, Complexity = 1075658, Validation Loss = 2.5089\n","Evaluating individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'dense', 'units': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5570, Complexity = 301578, Validation Loss = 2.1146\n","Evaluating individual: [{'type': 'conv', 'filters': 16, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}]\n","Results for individual: Accuracy = 0.5655, Complexity = 50698, Validation Loss = 1.5768\n","Individual Fitness: (0.5765, 1090122, 1.989516209988367)\n","Individual Fitness: (0.5575, 77274, 1.6716356532914298)\n","Individual Fitness: (0.556, 78298, 1.447691381923736)\n","Individual Fitness: (0.549, 50698, 1.5757585423333305)\n","Individual Fitness: (0.5735, 50698, 1.3542041258206443)\n","Individual Fitness: (0.4025, 215882, 1.6831644735639057)\n","Individual Fitness: (0.5675, 22298, 1.4481812337088207)\n","Individual Fitness: (0.55, 1075658, 2.5088817699561043)\n","Individual Fitness: (0.557, 301578, 2.1146265760300653)\n","Individual Fitness: (0.5655, 50698, 1.5767736652540782)\n","Best Individual: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}], Fitness: (0.5765, 1090122.0, 1.989516209988367)\n","Best architecture: [{'type': 'conv', 'filters': 64, 'activation': 'relu'}, {'type': 'conv', 'filters': 64, 'activation': 'tanh'}, {'type': 'dense', 'units': 256, 'activation': 'tanh'}, {'type': 'conv', 'filters': 32, 'activation': 'tanh'}]\n"]}]},{"cell_type":"code","source":["def test_model(architecture):\n","    model = CustomCNN(architecture)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x_batch, y_batch in test_loader:\n","            outputs = model(x_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","    print(f\"Test Accuracy: {correct / total:.4f}\")\n","\n","test_model(best_architecture)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFeUHro2GDGZ","outputId":"51c9d6f6-997e-4935-9035-dd6c711ffaf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.0944\n"]}]},{"cell_type":"code","source":["class CustomBaselineCNN(nn.Module):\n","    def __init__(self):\n","        super(CustomBaselineCNN, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Flatten(),\n","            nn.Linear(64 * 8 * 8, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 10)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"xOBiXjHNSj9q","executionInfo":{"status":"error","timestamp":1733007403255,"user_tz":300,"elapsed":302,"user":{"displayName":"Rahul shah","userId":"08697968008726674087"}},"outputId":"8a4dfc12-5989-4a57-8b79-1d288ee0544a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d2f74304a497>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomBaselineCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomBaselineCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         self.model = nn.Sequential(\n\u001b[1;32m      5\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    early_stopping = EarlyStopping(patience=5)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(x_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation loss\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in val_loader:\n","                outputs = model(x_batch)\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","        val_loss /= len(val_loader)\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","            break\n","\n","    # Final accuracy on validation set\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x_batch, y_batch in val_loader:\n","            outputs = model(x_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    accuracy = correct / total\n","    complexity = sum(p.numel() for p in model.parameters())\n","    print(f\"Custom CNN: Accuracy = {accuracy:.4f}, Complexity = {complexity}, Validation Loss = {val_loss:.4f}\")\n","    return accuracy, complexity, val_loss\n"],"metadata":{"id":"MrxkuSS-datt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the custom CNN\n","custom_cnn = CustomBaselineCNN()\n","custom_results = train_model(custom_cnn, train_loader, val_loader, epochs=50)\n","\n","# Evaluate the best architecture from the evolutionary algorithm\n","best_evolved_model = CustomCNN(best_architecture)\n","evolved_results = evaluate_cnn(best_architecture)\n","\n","# Compare results\n","results = {\n","    \"Model\": [\"Custom CNN\", \"Evolved CNN\"],\n","    \"Accuracy\": [custom_results[0], evolved_results[0]],\n","    \"Complexity\": [custom_results[1], evolved_results[1]],\n","    \"Validation Loss\": [custom_results[2], evolved_results[2]]\n","}\n","\n","import pandas as pd\n","df_results = pd.DataFrame(results)\n","import ace_tools as tools\n","tools.display_dataframe_to_user(name=\"Model Comparison Results\", dataframe=df_results)\n"],"metadata":{"id":"SsJal0_XTviG"},"execution_count":null,"outputs":[]}]}